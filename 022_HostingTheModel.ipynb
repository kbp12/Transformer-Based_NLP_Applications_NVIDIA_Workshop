{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Hosting the Model\n",
    "\n",
    "In this notebook, you'll learn strategies to optimize Triton Server to improve the performance of your deployment.\n",
    "\n",
    "\n",
    "**[2.1 Concurrent Model Execution](#2.1-Concurrent-Model-Execution)**<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.1.1 Exercise: Usage Considerations](#2.1.1-Exercise:-Usage-Considerations)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.1.2 Implementation](#2.1.2-Implementation)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.1.3 Exercise: Configure Multiple Instance Groups](#2.1.3-Exercise:-Configure-Multiple-Instance-Groups)<br>\n",
    "**[2.2 Scheduling Strategies](#2.2-Scheduling-Strategies)**<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.2.1 Stateless Inference](#2.2.1-Stateless-Inference)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.2.2 Stateful Inference](#2.2.2-Stateful-Inference)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.2.3 Pipelines / Ensembles](#2.2.3-Pipelines-/-Ensembles)<br>\n",
    "**[2.3 Dynamic Batching](#2.3-Dynamic-Batching)**<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.3.1 Exercise: Implement Dynamic Batching](#2.3.1-Exercise:-Implement-Dynamic-Batching)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we've executed customer requests sequentially, in the order they have arrived at the server, and used a static batch of size 8 for any requests to our server. This has not only left our GPUs heavily underutilized, but has also significantly affected the latency of responses received from the server. This is not an uncommon situation. Unless you are developing an application that processes large volumes of data in batch, you will likely be sending individual inference requests from the user application, leading to even further underutilization. As we have seen in the previous notebook, model optimizations do help considerably to accelerate model execution.  However, they do not change the fact that when serving is implemented naively, the nature of the inference workload leads to GPU underutilization.\n",
    "\n",
    "Inference servers, such as NVIDIA Triton, implement a wide range of features that allow us to improve the GPU utilization and improve request latency. The three that we will discuss in this class are:<br/>\n",
    "- <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/architecture.html#section-concurrent-model-execution\">Concurrent model execution</a></br>\n",
    "- <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/models_and_schedulers.html\">Scheduling</a> <br/>\n",
    "- <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/model_configuration.html#section-dynamic-batcher\">Dynamic batching</a> <br/>\n",
    "\n",
    "\n",
    "Please refer to the <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/quickstart.html\">Triton documentation</a> and its <a href=\"https://github.com/NVIDIA/triton-inference-server\">source code</a> for further information about the mechanisms and configurations that can help improve model inference performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Concurrent Model Execution\n",
    "The Triton architecture allows multiple models and/or multiple instances of the same model to execute in parallel on a single GPU. The following figure shows an example with two models: `model0` and `model1`. Assuming Triton is not currently processing any request, when two requests arrive simultaneously, one for each model, Triton immediately schedules both of them onto the GPU, and the GPUâ€™s hardware scheduler begins working on both computations in parallel. </br>\n",
    "\n",
    "<img src=\"images/multi_model_exec.png\"/><br/>\n",
    "\n",
    "#### Default Behavior\n",
    "\n",
    "By default, if multiple requests for the same model arrive at the same time, Triton will serialize their execution by scheduling only one at a time on the GPU, as shown in the following figure.\n",
    "\n",
    "<img src=\"images/multi_model_serial_exec.png\"/><br/>\n",
    "\n",
    "Triton provides an instance-group feature that allows each model to specify how many parallel executions of that model should be allowed. Each such enabled parallel execution is referred to as an *execution instance*. By default, Triton gives each model a single execution instance, which means that only a single execution of the model is allowed to be in progress at a time as shown in the above figure. \n",
    "\n",
    "#### Instance Groups\n",
    "By using the *instance-group* setting, the number of execution instances for a model can be increased. The following figure shows model execution when `model1` is configured to allow three execution instances. As shown in the figure, the first three `model1` inference requests are immediately executed in parallel on the GPU. The fourth `model1` inference request must wait until one of the first three executions completes before beginning.\n",
    "\n",
    "<img src=\"images/multi_model_parallel_exec.png\"/><br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.1 Exercise: Usage Considerations\n",
    "\n",
    "For most models, the Triton feature that provides the largest performance improvement is *dynamic batching*. The key advantages of dynamic batching over setting up multiple instance execution are:\n",
    "- No overhead for model parameter storage\n",
    "- No overhead related to model parameter fetch from the GPU memory\n",
    "- Better utilization of the GPU resources\n",
    "\n",
    "Before we look at the configuration for multiple model execution, let's execute our model again using a single instance, and observe the resource utilization of the GPU. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise Steps\n",
    "1. Launch a terminal window from the JupyterLab launch page.  If you need to open a new launch page, click the '+' icon on the left sidebar menu. You can then use a drag-and-drop action to move the terminal to a sub-window configuration  for better viewing.\n",
    "2. Execute the following command in the terminal before you run the performance tool:<br>\n",
    "\n",
    "```\n",
    "watch -n0.5 nvidia-smi\n",
    "```\n",
    "    You should see an output that resembles:\n",
    "<img src=\"images/NVIDIASMI.png\" style=\"position:relative; left:30px;\" width=800/>\n",
    "\n",
    "3. Execute the same benchmark we used in the previous notebook, but with the batch size reduced to 1, and observe the <code>nvidia-smi</code> output again.  Pay special attention to the memory consumption and GPU utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n"
     ]
    }
   ],
   "source": [
    "# Set the server hostname and check it - you should get a message that \"Triton Server is ready!\"\n",
    "tritonServerHostName = \"triton\"\n",
    "!./utilities/wait_for_triton_server.sh {tritonServerHostName}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the previous configuration.\n",
    "modelVersion=\"1\"\n",
    "precision=\"fp32\"\n",
    "batchSize=\"1\"\n",
    "maxLatency=\"500\"\n",
    "maxClientThreads=\"10\"\n",
    "maxConcurrency=\"2\"\n",
    "dockerBridge=\"host\"\n",
    "resultsFolderName=\"1\"\n",
    "profilingData=\"utilities/profiling_data_int64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: bertQA-onnx-trt-fp16\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "WARNING: Overriding max_threads specification to ensure requested concurrency range.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Measurement window: 3000 msec\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 10 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 41.3333 infer/sec. Avg latency: 24179 usec (std 230 usec)\n",
      "  Pass [2] throughput: 41.6667 infer/sec. Avg latency: 24010 usec (std 44 usec)\n",
      "  Pass [3] throughput: 41.6667 infer/sec. Avg latency: 23993 usec (std 45 usec)\n",
      "  Client: \n",
      "    Request count: 125\n",
      "    Throughput: 41.6667 infer/sec\n",
      "    Avg latency: 23993 usec (standard deviation 45 usec)\n",
      "    p50 latency: 23990 usec\n",
      "    p90 latency: 24042 usec\n",
      "    p95 latency: 24062 usec\n",
      "    p99 latency: 24122 usec\n",
      "    Avg HTTP time: 23990 usec (send 4 usec + response wait 23985 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 150\n",
      "    Execution count: 150\n",
      "    Successful request count: 150\n",
      "    Avg request latency: 23682 usec (overhead 3 usec + queue 26 usec + compute input 11 usec + compute infer 23630 usec + compute output 12 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 42.3333 infer/sec. Avg latency: 47416 usec (std 69 usec)\n",
      "  Pass [2] throughput: 42 infer/sec. Avg latency: 47492 usec (std 105 usec)\n",
      "  Pass [3] throughput: 42.3333 infer/sec. Avg latency: 47534 usec (std 105 usec)\n",
      "  Client: \n",
      "    Request count: 127\n",
      "    Throughput: 42.3333 infer/sec\n",
      "    Avg latency: 47534 usec (standard deviation 105 usec)\n",
      "    p50 latency: 47536 usec\n",
      "    p90 latency: 47673 usec\n",
      "    p95 latency: 47722 usec\n",
      "    p99 latency: 47801 usec\n",
      "    Avg HTTP time: 47519 usec (send 7 usec + response wait 47510 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 151\n",
      "    Execution count: 151\n",
      "    Successful request count: 151\n",
      "    Avg request latency: 47141 usec (overhead 3 usec + queue 23470 usec + compute input 12 usec + compute infer 23642 usec + compute output 14 usec)\n",
      "\n",
      "Request concurrency: 3\n",
      "  Pass [1] throughput: 42 infer/sec. Avg latency: 71225 usec (std 128 usec)\n",
      "  Pass [2] throughput: 42.3333 infer/sec. Avg latency: 71268 usec (std 113 usec)\n",
      "  Pass [3] throughput: 42 infer/sec. Avg latency: 71223 usec (std 90 usec)\n",
      "  Client: \n",
      "    Request count: 126\n",
      "    Throughput: 42 infer/sec\n",
      "    Avg latency: 71223 usec (standard deviation 90 usec)\n",
      "    p50 latency: 71221 usec\n",
      "    p90 latency: 71363 usec\n",
      "    p95 latency: 71380 usec\n",
      "    p99 latency: 71426 usec\n",
      "    Avg HTTP time: 71212 usec (send 6 usec + response wait 71205 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 151\n",
      "    Execution count: 151\n",
      "    Successful request count: 151\n",
      "    Avg request latency: 70833 usec (overhead 3 usec + queue 47175 usec + compute input 10 usec + compute infer 23634 usec + compute output 11 usec)\n",
      "\n",
      "Request concurrency: 4\n",
      "  Pass [1] throughput: 42.3333 infer/sec. Avg latency: 94916 usec (std 84 usec)\n",
      "  Pass [2] throughput: 42 infer/sec. Avg latency: 94944 usec (std 118 usec)\n",
      "  Pass [3] throughput: 42.3333 infer/sec. Avg latency: 94948 usec (std 118 usec)\n",
      "  Client: \n",
      "    Request count: 127\n",
      "    Throughput: 42.3333 infer/sec\n",
      "    Avg latency: 94948 usec (standard deviation 118 usec)\n",
      "    p50 latency: 94958 usec\n",
      "    p90 latency: 95083 usec\n",
      "    p95 latency: 95150 usec\n",
      "    p99 latency: 95188 usec\n",
      "    Avg HTTP time: 94950 usec (send 6 usec + response wait 94943 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 151\n",
      "    Execution count: 151\n",
      "    Successful request count: 151\n",
      "    Avg request latency: 94575 usec (overhead 4 usec + queue 70920 usec + compute input 11 usec + compute infer 23629 usec + compute output 11 usec)\n",
      "\n",
      "Request concurrency: 5\n",
      "  Pass [1] throughput: 42 infer/sec. Avg latency: 118726 usec (std 128 usec)\n",
      "  Pass [2] throughput: 42.3333 infer/sec. Avg latency: 118660 usec (std 117 usec)\n",
      "  Pass [3] throughput: 42 infer/sec. Avg latency: 118711 usec (std 131 usec)\n",
      "  Client: \n",
      "    Request count: 126\n",
      "    Throughput: 42 infer/sec\n",
      "    Avg latency: 118711 usec (standard deviation 131 usec)\n",
      "    p50 latency: 118719 usec\n",
      "    p90 latency: 118899 usec\n",
      "    p95 latency: 118929 usec\n",
      "    p99 latency: 119043 usec\n",
      "    Avg HTTP time: 118671 usec (send 7 usec + response wait 118663 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 151\n",
      "    Execution count: 151\n",
      "    Successful request count: 151\n",
      "    Avg request latency: 118291 usec (overhead 2 usec + queue 94641 usec + compute input 11 usec + compute infer 23625 usec + compute output 12 usec)\n",
      "\n",
      "Request concurrency: 6\n",
      "  Pass [1] throughput: 42.3333 infer/sec. Avg latency: 142423 usec (std 164 usec)\n",
      "  Pass [2] throughput: 42 infer/sec. Avg latency: 142598 usec (std 275 usec)\n",
      "  Pass [3] throughput: 42.3333 infer/sec. Avg latency: 142581 usec (std 211 usec)\n",
      "  Client: \n",
      "    Request count: 127\n",
      "    Throughput: 42.3333 infer/sec\n",
      "    Avg latency: 142581 usec (standard deviation 211 usec)\n",
      "    p50 latency: 142554 usec\n",
      "    p90 latency: 142870 usec\n",
      "    p95 latency: 143029 usec\n",
      "    p99 latency: 143252 usec\n",
      "    Avg HTTP time: 142613 usec (send 8 usec + response wait 142603 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 151\n",
      "    Execution count: 151\n",
      "    Successful request count: 151\n",
      "    Avg request latency: 142202 usec (overhead 3 usec + queue 118524 usec + compute input 12 usec + compute infer 23650 usec + compute output 13 usec)\n",
      "\n",
      "Request concurrency: 7\n",
      "  Pass [1] throughput: 42 infer/sec. Avg latency: 166270 usec (std 183 usec)\n",
      "  Pass [2] throughput: 42 infer/sec. Avg latency: 166398 usec (std 338 usec)\n",
      "  Pass [3] throughput: 42.3333 infer/sec. Avg latency: 166414 usec (std 253 usec)\n",
      "  Client: \n",
      "    Request count: 127\n",
      "    Throughput: 42.3333 infer/sec\n",
      "    Avg latency: 166414 usec (standard deviation 253 usec)\n",
      "    p50 latency: 166375 usec\n",
      "    p90 latency: 166747 usec\n",
      "    p95 latency: 166881 usec\n",
      "    p99 latency: 167001 usec\n",
      "    Avg HTTP time: 166408 usec (send 8 usec + response wait 166399 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 152\n",
      "    Execution count: 152\n",
      "    Successful request count: 152\n",
      "    Avg request latency: 166002 usec (overhead 3 usec + queue 142323 usec + compute input 13 usec + compute infer 23650 usec + compute output 13 usec)\n",
      "\n",
      "Request concurrency: 8\n",
      "  Pass [1] throughput: 42 infer/sec. Avg latency: 190079 usec (std 372 usec)\n",
      "  Pass [2] throughput: 42 infer/sec. Avg latency: 190019 usec (std 264 usec)\n",
      "  Pass [3] throughput: 42.3333 infer/sec. Avg latency: 189892 usec (std 173 usec)\n",
      "  Client: \n",
      "    Request count: 127\n",
      "    Throughput: 42.3333 infer/sec\n",
      "    Avg latency: 189892 usec (standard deviation 173 usec)\n",
      "    p50 latency: 189880 usec\n",
      "    p90 latency: 190158 usec\n",
      "    p95 latency: 190214 usec\n",
      "    p99 latency: 190245 usec\n",
      "    Avg HTTP time: 189885 usec (send 7 usec + response wait 189877 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 151\n",
      "    Execution count: 151\n",
      "    Successful request count: 151\n",
      "    Avg request latency: 189495 usec (overhead 3 usec + queue 165842 usec + compute input 11 usec + compute infer 23627 usec + compute output 12 usec)\n",
      "\n",
      "Request concurrency: 9\n",
      "  Pass [1] throughput: 42 infer/sec. Avg latency: 213640 usec (std 209 usec)\n",
      "  Pass [2] throughput: 42.3333 infer/sec. Avg latency: 213571 usec (std 160 usec)\n",
      "  Pass [3] throughput: 42 infer/sec. Avg latency: 213561 usec (std 161 usec)\n",
      "  Client: \n",
      "    Request count: 126\n",
      "    Throughput: 42 infer/sec\n",
      "    Avg latency: 213561 usec (standard deviation 161 usec)\n",
      "    p50 latency: 213585 usec\n",
      "    p90 latency: 213754 usec\n",
      "    p95 latency: 213811 usec\n",
      "    p99 latency: 213922 usec\n",
      "    Avg HTTP time: 213557 usec (send 7 usec + response wait 213549 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 151\n",
      "    Execution count: 151\n",
      "    Successful request count: 151\n",
      "    Avg request latency: 213174 usec (overhead 3 usec + queue 189528 usec + compute input 11 usec + compute infer 23620 usec + compute output 12 usec)\n",
      "\n",
      "Request concurrency: 10\n",
      "  Pass [1] throughput: 42.3333 infer/sec. Avg latency: 236977 usec (std 2832 usec)\n",
      "  Pass [2] throughput: 42 infer/sec. Avg latency: 237319 usec (std 155 usec)\n",
      "  Pass [3] throughput: 42.3333 infer/sec. Avg latency: 237515 usec (std 170 usec)\n",
      "  Client: \n",
      "    Request count: 127\n",
      "    Throughput: 42.3333 infer/sec\n",
      "    Avg latency: 237515 usec (standard deviation 170 usec)\n",
      "    p50 latency: 237525 usec\n",
      "    p90 latency: 237728 usec\n",
      "    p95 latency: 237758 usec\n",
      "    p99 latency: 237828 usec\n",
      "    Avg HTTP time: 237502 usec (send 8 usec + response wait 237493 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 151\n",
      "    Execution count: 151\n",
      "    Successful request count: 151\n",
      "    Avg request latency: 237109 usec (overhead 3 usec + queue 213444 usec + compute input 12 usec + compute infer 23638 usec + compute output 12 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 41.6667 infer/sec, latency 23993 usec\n",
      "Concurrency: 2, throughput: 42.3333 infer/sec, latency 47534 usec\n",
      "Concurrency: 3, throughput: 42 infer/sec, latency 71223 usec\n",
      "Concurrency: 4, throughput: 42.3333 infer/sec, latency 94948 usec\n",
      "Concurrency: 5, throughput: 42 infer/sec, latency 118711 usec\n",
      "Concurrency: 6, throughput: 42.3333 infer/sec, latency 142581 usec\n",
      "Concurrency: 7, throughput: 42.3333 infer/sec, latency 166414 usec\n",
      "Concurrency: 8, throughput: 42.3333 infer/sec, latency 189892 usec\n",
      "Concurrency: 9, throughput: 42 infer/sec, latency 213561 usec\n",
      "Concurrency: 10, throughput: 42.3333 infer/sec, latency 237515 usec\n"
     ]
    }
   ],
   "source": [
    "# Update configuration parameters and run profiler.\n",
    "modelName = \"bertQA-onnx-trt-fp16\"\n",
    "maxConcurrency= \"10\"\n",
    "batchSize=\"1\"\n",
    "print(\"Running: \" + modelName)\n",
    "!bash ./utilities/run_perf_client_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurrency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you have observed utilization similar to the following:<br/>\n",
    "<img src=\"images/NVIDIASMI2.png\" width=800/><br/>\n",
    "\n",
    "Do you think you will observe a major acceleration as a consequence of increasing the number of instance groups?<br>\n",
    "Discuss with the instructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.2 Implementation\n",
    "Let's look at how to enable concurrent execution and what impact it will have on our model performance. Execute the following code cells to export the model in the ONNX format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"bertQA-onnx-conexec\"\n",
    "exportFormat = \"onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploying model bertQA-onnx-conexec in format onnxruntime_onnx\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__0\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__1\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__2\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__0\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__1\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:604] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 1336539973\n",
      "\n",
      "conversion correctness test results\n",
      "-----------------------------------\n",
      "maximal absolute error over dataset (L_inf):  0.00022935867309570312\n",
      "\n",
      "average L_inf error over output tensors:  0.0001423954963684082\n",
      "variance of L_inf error over output tensors:  6.657553323445124e-09\n",
      "stddev of L_inf error over output tensors:  8.159383140559784e-05\n",
      "\n",
      "time of error check of native model:  0.436572790145874 seconds\n",
      "time of error check of onnx model:  23.12346339225769 seconds\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!python ./deployer/deployer.py \\\n",
    "    --{exportFormat} \\\n",
    "    --save-dir ./candidatemodels \\\n",
    "    --triton-model-name {modelName} \\\n",
    "    --triton-model-version 1 \\\n",
    "    --triton-max-batch-size 8 \\\n",
    "    --triton-dyn-batching-delay 0 \\\n",
    "    --triton-engine-count 1 \\\n",
    "    -- --checkpoint ./data/bert_qa.pt \\\n",
    "    --config_file ./bert_config.json \\\n",
    "    --vocab_file ./vocab \\\n",
    "    --predict_file ./squad/v1.1/dev-v1.1.json \\\n",
    "    --do_lower_case \\\n",
    "    --batch_size=8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16K\n",
      "drwxr-xr-x 3 root root 4.0K Aug  1 10:07 .\n",
      "drwxr-xr-x 3 root root 4.0K Aug  1 10:06 ..\n",
      "drwxr-xr-x 2 root root 4.0K Aug  1 10:06 1\n",
      "-rw-r--r-- 1 root root  569 Aug  1 10:07 config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "!ls -alh ./candidatemodels/bertQA-onnx-conexec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.3 Exercise: Configure Multiple Instance Groups\n",
    "In order to specify multiple instances, we need to change the \"count\" value from '1' to a larger number in the `instance_group` section of the \"config.pbtxt\" configuration file. \n",
    "\n",
    "\n",
    "```\n",
    "    instance_group [\n",
    "    {\n",
    "        count: 2\n",
    "        kind: KIND_GPU\n",
    "        gpus: [ 0 ]\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "#### Exercise Steps:\n",
    "1. Modify [config.pbtxt](candidatemodels/bertQA-onnx-conexec/config.pbtxt) in the `bertQA-onnx-conexec` deployment just created to specify two instances of our BERT-based question answering model. You should find the default instance_group block at the end of the file. Change the count variable from 1 to 2.  (see the [solution](solutions/ex-2-1-3_config.pbtxt) as needed)\n",
    "2. To make the comparison fair, also enable TensorRT with the addition of an `execution_accelerators` block inside the `optimization` block:\n",
    "\n",
    "```text\n",
    "optimization {\n",
    "   execution_accelerators {\n",
    "      gpu_execution_accelerator : [ {\n",
    "         name : \"tensorrt\"\n",
    "         parameters { key: \"precision_mode\" value: \"FP16\" }\n",
    "      }]\n",
    "   }\n",
    "cuda { graphs: 0 }\n",
    "}\n",
    "```\n",
    "\n",
    "3. Once you have saved your changes (Main menu: File -> Save File), move the model across to Triton by executing the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ./candidatemodels/bertQA-onnx-conexec model_repository/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Run our standard stress test against the model. Please compare it to the single instance execution.<br>\n",
    "   Did the throughput change?<br>\n",
    "   Did the latency change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: bertQA-onnx-conexec\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "WARNING: Overriding max_threads specification to ensure requested concurrency range.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Measurement window: 3000 msec\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 10 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 41.6667 infer/sec. Avg latency: 24028 usec (std 230 usec)\n",
      "  Pass [2] throughput: 41.3333 infer/sec. Avg latency: 24078 usec (std 148 usec)\n",
      "  Pass [3] throughput: 41.6667 infer/sec. Avg latency: 24004 usec (std 75 usec)\n",
      "  Client: \n",
      "    Request count: 125\n",
      "    Throughput: 41.6667 infer/sec\n",
      "    Avg latency: 24004 usec (standard deviation 75 usec)\n",
      "    p50 latency: 23985 usec\n",
      "    p90 latency: 24096 usec\n",
      "    p95 latency: 24134 usec\n",
      "    p99 latency: 24319 usec\n",
      "    Avg HTTP time: 23996 usec (send 6 usec + response wait 23989 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 150\n",
      "    Execution count: 150\n",
      "    Successful request count: 150\n",
      "    Avg request latency: 23651 usec (overhead 3 usec + queue 35 usec + compute input 11 usec + compute infer 23590 usec + compute output 12 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 42.3333 infer/sec. Avg latency: 47447 usec (std 98 usec)\n",
      "  Pass [2] throughput: 42 infer/sec. Avg latency: 47429 usec (std 85 usec)\n",
      "  Pass [3] throughput: 42.3333 infer/sec. Avg latency: 47554 usec (std 194 usec)\n",
      "  Client: \n",
      "    Request count: 127\n",
      "    Throughput: 42.3333 infer/sec\n",
      "    Avg latency: 47554 usec (standard deviation 194 usec)\n",
      "    p50 latency: 47520 usec\n",
      "    p90 latency: 47769 usec\n",
      "    p95 latency: 47977 usec\n",
      "    p99 latency: 48274 usec\n",
      "    Avg HTTP time: 47559 usec (send 7 usec + response wait 47550 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 151\n",
      "    Execution count: 151\n",
      "    Successful request count: 151\n",
      "    Avg request latency: 47183 usec (overhead 3 usec + queue 23492 usec + compute input 12 usec + compute infer 23663 usec + compute output 13 usec)\n",
      "\n",
      "Request concurrency: 3\n",
      "  Pass [1] throughput: 42 infer/sec. Avg latency: 71264 usec (std 149 usec)\n",
      "  Pass [2] throughput: 42.3333 infer/sec. Avg latency: 71177 usec (std 96 usec)\n",
      "  Pass [3] throughput: 42 infer/sec. Avg latency: 71245 usec (std 117 usec)\n",
      "  Client: \n",
      "    Request count: 126\n",
      "    Throughput: 42 infer/sec\n",
      "    Avg latency: 71245 usec (standard deviation 117 usec)\n",
      "    p50 latency: 71238 usec\n",
      "    p90 latency: 71409 usec\n",
      "    p95 latency: 71463 usec\n",
      "    p99 latency: 71523 usec\n",
      "    Avg HTTP time: 71254 usec (send 7 usec + response wait 71245 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 151\n",
      "    Execution count: 151\n",
      "    Successful request count: 151\n",
      "    Avg request latency: 70857 usec (overhead 2 usec + queue 47192 usec + compute input 12 usec + compute infer 23639 usec + compute output 12 usec)\n",
      "\n",
      "Request concurrency: 4\n",
      "  Pass [1] throughput: 42.3333 infer/sec. Avg latency: 94909 usec (std 89 usec)\n",
      "  Pass [2] throughput: 42 infer/sec. Avg latency: 94896 usec (std 114 usec)\n",
      "  Pass [3] throughput: 42 infer/sec. Avg latency: 94880 usec (std 141 usec)\n",
      "  Client: \n",
      "    Request count: 126\n",
      "    Throughput: 42 infer/sec\n",
      "    Avg latency: 94880 usec (standard deviation 141 usec)\n",
      "    p50 latency: 94848 usec\n",
      "    p90 latency: 95072 usec\n",
      "    p95 latency: 95146 usec\n",
      "    p99 latency: 95301 usec\n",
      "    Avg HTTP time: 94863 usec (send 7 usec + response wait 94855 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 152\n",
      "    Execution count: 152\n",
      "    Successful request count: 152\n",
      "    Avg request latency: 94488 usec (overhead 3 usec + queue 70854 usec + compute input 10 usec + compute infer 23609 usec + compute output 12 usec)\n",
      "\n",
      "Request concurrency: 5\n",
      "  Pass [1] throughput: 42 infer/sec. Avg latency: 118575 usec (std 136 usec)\n",
      "  Pass [2] throughput: 42 infer/sec. Avg latency: 118567 usec (std 117 usec)\n",
      "  Pass [3] throughput: 42.3333 infer/sec. Avg latency: 118571 usec (std 133 usec)\n",
      "  Client: \n",
      "    Request count: 127\n",
      "    Throughput: 42.3333 infer/sec\n",
      "    Avg latency: 118571 usec (standard deviation 133 usec)\n",
      "    p50 latency: 118561 usec\n",
      "    p90 latency: 118685 usec\n",
      "    p95 latency: 118804 usec\n",
      "    p99 latency: 119083 usec\n",
      "    Avg HTTP time: 118561 usec (send 7 usec + response wait 118553 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 152\n",
      "    Execution count: 152\n",
      "    Successful request count: 152\n",
      "    Avg request latency: 118183 usec (overhead 3 usec + queue 94551 usec + compute input 11 usec + compute infer 23607 usec + compute output 11 usec)\n",
      "\n",
      "Request concurrency: 6\n",
      "  Pass [1] throughput: 42.3333 infer/sec. Avg latency: 142355 usec (std 128 usec)\n",
      "  Pass [2] throughput: 42 infer/sec. Avg latency: 142463 usec (std 153 usec)\n",
      "  Pass [3] throughput: 42.3333 infer/sec. Avg latency: 142624 usec (std 428 usec)\n",
      "  Client: \n",
      "    Request count: 127\n",
      "    Throughput: 42.3333 infer/sec\n",
      "    Avg latency: 142624 usec (standard deviation 428 usec)\n",
      "    p50 latency: 142514 usec\n",
      "    p90 latency: 143213 usec\n",
      "    p95 latency: 143409 usec\n",
      "    p99 latency: 143754 usec\n",
      "    Avg HTTP time: 142729 usec (send 8 usec + response wait 142719 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 151\n",
      "    Execution count: 151\n",
      "    Successful request count: 151\n",
      "    Avg request latency: 142299 usec (overhead 3 usec + queue 118608 usec + compute input 14 usec + compute infer 23659 usec + compute output 15 usec)\n",
      "\n",
      "Request concurrency: 7\n",
      "  Pass [1] throughput: 42 infer/sec. Avg latency: 166167 usec (std 437 usec)\n",
      "  Pass [2] throughput: 42 infer/sec. Avg latency: 166083 usec (std 185 usec)\n",
      "  Pass [3] throughput: 42.3333 infer/sec. Avg latency: 166024 usec (std 172 usec)\n",
      "  Client: \n",
      "    Request count: 127\n",
      "    Throughput: 42.3333 infer/sec\n",
      "    Avg latency: 166024 usec (standard deviation 172 usec)\n",
      "    p50 latency: 166040 usec\n",
      "    p90 latency: 166230 usec\n",
      "    p95 latency: 166317 usec\n",
      "    p99 latency: 166477 usec\n",
      "    Avg HTTP time: 166022 usec (send 8 usec + response wait 166013 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 152\n",
      "    Execution count: 152\n",
      "    Successful request count: 152\n",
      "    Avg request latency: 165632 usec (overhead 3 usec + queue 142000 usec + compute input 12 usec + compute infer 23605 usec + compute output 12 usec)\n",
      "\n",
      "Request concurrency: 8\n",
      "  Pass [1] throughput: 42.3333 infer/sec. Avg latency: 189704 usec (std 139 usec)\n",
      "  Pass [2] throughput: 42 infer/sec. Avg latency: 190109 usec (std 449 usec)\n",
      "  Pass [3] throughput: 42.3333 infer/sec. Avg latency: 190221 usec (std 452 usec)\n",
      "  Client: \n",
      "    Request count: 127\n",
      "    Throughput: 42.3333 infer/sec\n",
      "    Avg latency: 190221 usec (standard deviation 452 usec)\n",
      "    p50 latency: 190111 usec\n",
      "    p90 latency: 190811 usec\n",
      "    p95 latency: 191214 usec\n",
      "    p99 latency: 191398 usec\n",
      "    Avg HTTP time: 190212 usec (send 8 usec + response wait 190202 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 151\n",
      "    Execution count: 151\n",
      "    Successful request count: 151\n",
      "    Avg request latency: 189786 usec (overhead 4 usec + queue 166107 usec + compute input 13 usec + compute infer 23648 usec + compute output 14 usec)\n",
      "\n",
      "Request concurrency: 9\n",
      "  Pass [1] throughput: 42 infer/sec. Avg latency: 213686 usec (std 153 usec)\n",
      "  Pass [2] throughput: 42 infer/sec. Avg latency: 213786 usec (std 320 usec)\n",
      "  Pass [3] throughput: 42.3333 infer/sec. Avg latency: 213770 usec (std 323 usec)\n",
      "  Client: \n",
      "    Request count: 127\n",
      "    Throughput: 42.3333 infer/sec\n",
      "    Avg latency: 213770 usec (standard deviation 323 usec)\n",
      "    p50 latency: 213697 usec\n",
      "    p90 latency: 214268 usec\n",
      "    p95 latency: 214434 usec\n",
      "    p99 latency: 214565 usec\n",
      "    Avg HTTP time: 213743 usec (send 8 usec + response wait 213733 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 152\n",
      "    Execution count: 152\n",
      "    Successful request count: 152\n",
      "    Avg request latency: 213340 usec (overhead 2 usec + queue 189680 usec + compute input 12 usec + compute infer 23633 usec + compute output 13 usec)\n",
      "\n",
      "Request concurrency: 10\n",
      "  Pass [1] throughput: 42 infer/sec. Avg latency: 237305 usec (std 2274 usec)\n",
      "  Pass [2] throughput: 42 infer/sec. Avg latency: 237446 usec (std 272 usec)\n",
      "  Pass [3] throughput: 42 infer/sec. Avg latency: 237362 usec (std 186 usec)\n",
      "  Client: \n",
      "    Request count: 126\n",
      "    Throughput: 42 infer/sec\n",
      "    Avg latency: 237362 usec (standard deviation 186 usec)\n",
      "    p50 latency: 237352 usec\n",
      "    p90 latency: 237661 usec\n",
      "    p95 latency: 237715 usec\n",
      "    p99 latency: 237770 usec\n",
      "    Avg HTTP time: 237333 usec (send 7 usec + response wait 237325 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 152\n",
      "    Execution count: 152\n",
      "    Successful request count: 152\n",
      "    Avg request latency: 236950 usec (overhead 3 usec + queue 213302 usec + compute input 12 usec + compute infer 23621 usec + compute output 12 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 41.6667 infer/sec, latency 24004 usec\n",
      "Concurrency: 2, throughput: 42.3333 infer/sec, latency 47554 usec\n",
      "Concurrency: 3, throughput: 42 infer/sec, latency 71245 usec\n",
      "Concurrency: 4, throughput: 42 infer/sec, latency 94880 usec\n",
      "Concurrency: 5, throughput: 42.3333 infer/sec, latency 118571 usec\n",
      "Concurrency: 6, throughput: 42.3333 infer/sec, latency 142624 usec\n",
      "Concurrency: 7, throughput: 42.3333 infer/sec, latency 166024 usec\n",
      "Concurrency: 8, throughput: 42.3333 infer/sec, latency 190221 usec\n",
      "Concurrency: 9, throughput: 42.3333 infer/sec, latency 213770 usec\n",
      "Concurrency: 10, throughput: 42 infer/sec, latency 237362 usec\n"
     ]
    }
   ],
   "source": [
    "maxConcurrency= \"10\"\n",
    "batchSize=\"1\"\n",
    "print(\"Running: \" + modelName)\n",
    "!bash ./utilities/run_perf_client_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurrency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue, let's free up some GPU memory by moving some of the models out of the Triton model repository.  After removing the following three models, only the `bertQA-torchscript` model should remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bertQA-torchscript\n"
     ]
    }
   ],
   "source": [
    "# Remove models from the inference server by removing them from the model_repository\n",
    "!mv /dli/task/model_repository/bertQA-onnx /dli/task/candidatemodels/\n",
    "!mv /dli/task/model_repository/bertQA-onnx-conexec /dli/task/candidatemodels/\n",
    "!mv /dli/task/model_repository/bertQA-onnx-trt-fp16 /dli/task/candidatemodels/\n",
    "\n",
    "# List remaining models on the inference server\n",
    "!ls /dli/task/model_repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Scheduling Strategies\n",
    "Triton supports batch inferencing by allowing individual inference requests to specify a batch of inputs. The inferencing for a batch of inputs is performed at the same time which is especially important for GPUs since it can greatly increase inferencing throughput. In many use cases the individual inference requests are not batched, therefore, they do not benefit from the throughput benefits of batching. <br/>\n",
    "\n",
    "The inference server contains multiple scheduling and batching algorithms that support many different model types and use-cases. The choice of the scheduler / batcher will be driven by several factors the key ones being:\n",
    "- Stateful / stateless nature of your inference workload\n",
    "- Whether your application is composed of models served in isolation or whether a more complex pipeline / ensemble is being used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.1 Stateless Inference\n",
    "\n",
    "When dealing with stateless inference (as we are in this class) we have two main options when it comes to scheduling. The first option is the default scheduler which will distribute request to all instances assigned for inference. This is the preferred option when the structure of the inference workload is well understood and where inference will take place at regular batch sizes and time intervals.\n",
    "\n",
    "The second option is dynamic batching which combines individual request and similarly to the default batcher distributes the larges batches across instances. We will discuss this particular option in the next section of the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.2 Stateful Inference\n",
    "\n",
    "A stateful model (or stateful custom backend) does maintain state between inference requests. The model is expecting multiple inference requests that together form a sequence of inferences that must be routed to the same model instance so that the state being maintained by the model is correctly updated. Moreover, the model may require that Triton provide control signals indicating, for example, sequence start.\n",
    "\n",
    "The sequence batcher can employ one of two scheduling strategies when deciding how to batch the sequences that are routed to the same model instance. These strategies are Direct and Oldest.\n",
    "\n",
    "With the Direct scheduling strategy the sequence batcher ensures not only that all inference requests in a sequence are routed to the same model instance, but also that each sequence is routed to a dedicated batch slot within the model instance. This strategy is required when the model maintains state for each batch slot, and is expecting all inference requests for a given sequence to be routed to the same slot so that the state is correctly updated.\n",
    "\n",
    "With the Oldest scheduling strategy the sequence batcher ensures that all inference requests in a sequence are routed to the same model instance and then uses the dynamic batcher to batch together multiple inferences from different sequences into a batch that inferences together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.3 Pipelines / Ensembles\n",
    "\n",
    "An ensemble model represents a pipeline of one or more models and the connection of input and output tensors between those models. Ensemble models are intended to be used to encapsulate a procedure that involves multiple models, such as \"data preprocessing -> inference -> data post-processing\". Using ensemble models for this purpose can avoid the overhead of transferring intermediate tensors and minimize the number of requests that must be sent to Triton. An example of an ensemble pipeline is illustrated below: <br/>\n",
    "\n",
    "<img src=\"images/ensemble_example0.png\"/>\n",
    "\n",
    "The ensemble scheduler must be used for ensemble models, regardless of the scheduler used by the models within the ensemble. With respect to the ensemble scheduler, an ensemble model is not an actual model. Instead, it specifies the data flow between models within the ensemble as Step. The scheduler collects the output tensors in each step, provides them as input tensors for other steps according to the specification. In spite of that, the ensemble model is still viewed as a single model from an external view.\n",
    "\n",
    "More information on Triton scheduling can be found in the <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/models_and_schedulers.html#stateless-models\">following section of the documentation</a>. In this class, we will focus further on one of the most powerful features of Triton, *dynamic batching*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Dynamic Batching\n",
    "Dynamic batching is a feature of Triton that allows inference requests to be combined by the server, so that a batch is created dynamically, resulting in increased throughput.\n",
    "\n",
    "When a model instance becomes available for inferencing, the dynamic batcher will attempt to create batches from the requests that are available in the scheduler. Requests are added to the batch in the order the requests were received. If the dynamic batcher can form a batch of a preferred size(s) it will create a batch of the largest possible preferred size and send it for inferencing. If the dynamic batcher cannot form a batch of a preferred size, it will send a batch of the largest size possible that is less than the max batch size allowed by the model. \n",
    "\n",
    "The dynamic batcher can be configured to allow requests to be delayed for a limited time in the scheduler to allow other requests to join the dynamic batch. For example, the following configuration sets the maximum delay time of 100 microseconds for a request:\n",
    "\n",
    "```\n",
    "dynamic_batching {\n",
    "  preferred_batch_size: [ 4, 8 ]\n",
    "  max_queue_delay_microseconds: 100\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.1 Exercise: Implement Dynamic Batching\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin again by exporting an ONNX model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"bertQA-onnx-trt-dynbatch\"\n",
    "exportFormat = \"onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploying model bertQA-onnx-trt-dynbatch in format onnxruntime_onnx\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__0\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__1\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__2\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__0\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__1\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:604] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 1336539973\n",
      "\n",
      "conversion correctness test results\n",
      "-----------------------------------\n",
      "maximal absolute error over dataset (L_inf):  0.00022935867309570312\n",
      "\n",
      "average L_inf error over output tensors:  0.0001423954963684082\n",
      "variance of L_inf error over output tensors:  6.657553323445124e-09\n",
      "stddev of L_inf error over output tensors:  8.159383140559784e-05\n",
      "\n",
      "time of error check of native model:  0.43147826194763184 seconds\n",
      "time of error check of onnx model:  19.075185537338257 seconds\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!python ./deployer/deployer.py \\\n",
    "    --{exportFormat} \\\n",
    "    --save-dir ./candidatemodels \\\n",
    "    --triton-model-name {modelName} \\\n",
    "    --triton-model-version 1 \\\n",
    "    --triton-max-batch-size 8 \\\n",
    "    --triton-dyn-batching-delay 0 \\\n",
    "    --triton-engine-count 1 \\\n",
    "    -- --checkpoint ./data/bert_qa.pt \\\n",
    "    --config_file ./bert_config.json \\\n",
    "    --vocab_file ./vocab \\\n",
    "    --predict_file ./squad/v1.1/dev-v1.1.json \\\n",
    "    --do_lower_case \\\n",
    "    --batch_size=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise Steps\n",
    "1. Modify [config.pbtxt](candidatemodels/bertQA-onnx-trt-dynbatch/config.pbtxt) for dynamic batching using the example snippet. \n",
    "\n",
    "    ```\n",
    "    dynamic_batching {\n",
    "      preferred_batch_size: [ 4, 8 ]\n",
    "      max_queue_delay_microseconds: 100\n",
    "    }\n",
    "    ```\n",
    "    \n",
    "2. Enable TensorRT in the optimization block.\n",
    "\n",
    "    ```\n",
    "    optimization {\n",
    "       execution_accelerators {\n",
    "          gpu_execution_accelerator : [ {\n",
    "             name : \"tensorrt\"\n",
    "             parameters { key: \"precision_mode\" value: \"FP16\" }\n",
    "          }]\n",
    "       }\n",
    "    cuda { graphs: 0 }\n",
    "    }\n",
    "    ```\n",
    "3. Once saved, move the model to the Triton model repository and run the performance utility by executing the following cells. ([solution](solutions/ex-2-3-1_config.pbtxt) if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ./candidatemodels/bertQA-onnx-trt-dynbatch model_repository/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: bertQA-onnx-trt-dynbatch\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "WARNING: Overriding max_threads specification to ensure requested concurrency range.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Measurement window: 3000 msec\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 10 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 40 infer/sec. Avg latency: 25068 usec (std 985 usec)\n",
      "  Pass [2] throughput: 40.6667 infer/sec. Avg latency: 24586 usec (std 988 usec)\n",
      "  Pass [3] throughput: 41 infer/sec. Avg latency: 24343 usec (std 844 usec)\n",
      "  Client: \n",
      "    Request count: 123\n",
      "    Throughput: 41 infer/sec\n",
      "    Avg latency: 24343 usec (standard deviation 844 usec)\n",
      "    p50 latency: 23938 usec\n",
      "    p90 latency: 25914 usec\n",
      "    p95 latency: 26009 usec\n",
      "    p99 latency: 27273 usec\n",
      "    Avg HTTP time: 24549 usec (send 5 usec + response wait 24543 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 146\n",
      "    Execution count: 146\n",
      "    Successful request count: 146\n",
      "    Avg request latency: 24241 usec (overhead 4 usec + queue 28 usec + compute input 10 usec + compute infer 24188 usec + compute output 11 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 39 infer/sec. Avg latency: 51234 usec (std 760 usec)\n",
      "  Pass [2] throughput: 39 infer/sec. Avg latency: 51330 usec (std 1380 usec)\n",
      "  Pass [3] throughput: 39 infer/sec. Avg latency: 51539 usec (std 755 usec)\n",
      "  Client: \n",
      "    Request count: 117\n",
      "    Throughput: 39 infer/sec\n",
      "    Avg latency: 51539 usec (standard deviation 755 usec)\n",
      "    p50 latency: 51412 usec\n",
      "    p90 latency: 51931 usec\n",
      "    p95 latency: 53566 usec\n",
      "    p99 latency: 53740 usec\n",
      "    Avg HTTP time: 51550 usec (send 8 usec + response wait 51541 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 139\n",
      "    Execution count: 139\n",
      "    Successful request count: 139\n",
      "    Avg request latency: 51159 usec (overhead 3 usec + queue 25470 usec + compute input 12 usec + compute infer 25659 usec + compute output 15 usec)\n",
      "\n",
      "Request concurrency: 3\n",
      "  Pass [1] throughput: 39 infer/sec. Avg latency: 77090 usec (std 1429 usec)\n",
      "  Pass [2] throughput: 39 infer/sec. Avg latency: 76909 usec (std 1797 usec)\n",
      "  Pass [3] throughput: 39.3333 infer/sec. Avg latency: 76790 usec (std 1453 usec)\n",
      "  Client: \n",
      "    Request count: 118\n",
      "    Throughput: 39.3333 infer/sec\n",
      "    Avg latency: 76790 usec (standard deviation 1453 usec)\n",
      "    p50 latency: 76977 usec\n",
      "    p90 latency: 77196 usec\n",
      "    p95 latency: 77250 usec\n",
      "    p99 latency: 80973 usec\n",
      "    Avg HTTP time: 76804 usec (send 6 usec + response wait 76796 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 141\n",
      "    Execution count: 141\n",
      "    Successful request count: 141\n",
      "    Avg request latency: 76451 usec (overhead 3 usec + queue 50929 usec + compute input 9 usec + compute infer 25498 usec + compute output 12 usec)\n",
      "\n",
      "Request concurrency: 4\n",
      "  Pass [1] throughput: 39 infer/sec. Avg latency: 102445 usec (std 2172 usec)\n",
      "  Pass [2] throughput: 38.6667 infer/sec. Avg latency: 103027 usec (std 2632 usec)\n",
      "  Pass [3] throughput: 39 infer/sec. Avg latency: 102482 usec (std 2955 usec)\n",
      "  Client: \n",
      "    Request count: 117\n",
      "    Throughput: 39 infer/sec\n",
      "    Avg latency: 102482 usec (standard deviation 2955 usec)\n",
      "    p50 latency: 102787 usec\n",
      "    p90 latency: 105446 usec\n",
      "    p95 latency: 106725 usec\n",
      "    p99 latency: 108756 usec\n",
      "    Avg HTTP time: 102419 usec (send 7 usec + response wait 102411 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 141\n",
      "    Execution count: 141\n",
      "    Successful request count: 141\n",
      "    Avg request latency: 102052 usec (overhead 3 usec + queue 76514 usec + compute input 11 usec + compute infer 25512 usec + compute output 12 usec)\n",
      "\n",
      "Request concurrency: 5\n",
      "  Pass [1] throughput: 38.6667 infer/sec. Avg latency: 128819 usec (std 2320 usec)\n",
      "  Pass [2] throughput: 39 infer/sec. Avg latency: 128476 usec (std 2159 usec)\n",
      "  Pass [3] throughput: 39 infer/sec. Avg latency: 129070 usec (std 2315 usec)\n",
      "  Client: \n",
      "    Request count: 117\n",
      "    Throughput: 39 infer/sec\n",
      "    Avg latency: 129070 usec (standard deviation 2315 usec)\n",
      "    p50 latency: 128920 usec\n",
      "    p90 latency: 132593 usec\n",
      "    p95 latency: 133052 usec\n",
      "    p99 latency: 133466 usec\n",
      "    Avg HTTP time: 129008 usec (send 9 usec + response wait 128997 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 140\n",
      "    Execution count: 140\n",
      "    Successful request count: 140\n",
      "    Avg request latency: 128585 usec (overhead 3 usec + queue 102894 usec + compute input 14 usec + compute infer 25658 usec + compute output 16 usec)\n",
      "\n",
      "Request concurrency: 6\n",
      "  Pass [1] throughput: 38.6667 infer/sec. Avg latency: 154471 usec (std 2437 usec)\n",
      "  Pass [2] throughput: 39.3333 infer/sec. Avg latency: 152356 usec (std 3385 usec)\n",
      "  Pass [3] throughput: 39 infer/sec. Avg latency: 153119 usec (std 3480 usec)\n",
      "  Client: \n",
      "    Request count: 117\n",
      "    Throughput: 39 infer/sec\n",
      "    Avg latency: 153119 usec (standard deviation 3480 usec)\n",
      "    p50 latency: 153705 usec\n",
      "    p90 latency: 157672 usec\n",
      "    p95 latency: 158113 usec\n",
      "    p99 latency: 161459 usec\n",
      "    Avg HTTP time: 153181 usec (send 7 usec + response wait 153173 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 141\n",
      "    Execution count: 141\n",
      "    Successful request count: 141\n",
      "    Avg request latency: 152807 usec (overhead 3 usec + queue 127355 usec + compute input 11 usec + compute infer 25426 usec + compute output 12 usec)\n",
      "\n",
      "Request concurrency: 7\n",
      "  Pass [1] throughput: 39 infer/sec. Avg latency: 179097 usec (std 2834 usec)\n",
      "  Pass [2] throughput: 39.3333 infer/sec. Avg latency: 178275 usec (std 2154 usec)\n",
      "  Pass [3] throughput: 39 infer/sec. Avg latency: 180211 usec (std 2362 usec)\n",
      "  Client: \n",
      "    Request count: 117\n",
      "    Throughput: 39 infer/sec\n",
      "    Avg latency: 180211 usec (standard deviation 2362 usec)\n",
      "    p50 latency: 179625 usec\n",
      "    p90 latency: 183702 usec\n",
      "    p95 latency: 184124 usec\n",
      "    p99 latency: 185916 usec\n",
      "    Avg HTTP time: 179719 usec (send 7 usec + response wait 179711 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 140\n",
      "    Execution count: 140\n",
      "    Successful request count: 140\n",
      "    Avg request latency: 179347 usec (overhead 3 usec + queue 153756 usec + compute input 11 usec + compute infer 25565 usec + compute output 12 usec)\n",
      "\n",
      "Request concurrency: 8\n",
      "  Pass [1] throughput: 39 infer/sec. Avg latency: 205967 usec (std 3865 usec)\n",
      "  Pass [2] throughput: 39 infer/sec. Avg latency: 204945 usec (std 1776 usec)\n",
      "  Pass [3] throughput: 39 infer/sec. Avg latency: 205643 usec (std 2507 usec)\n",
      "  Client: \n",
      "    Request count: 117\n",
      "    Throughput: 39 infer/sec\n",
      "    Avg latency: 205643 usec (standard deviation 2507 usec)\n",
      "    p50 latency: 205614 usec\n",
      "    p90 latency: 208594 usec\n",
      "    p95 latency: 209588 usec\n",
      "    p99 latency: 211554 usec\n",
      "    Avg HTTP time: 205209 usec (send 8 usec + response wait 205200 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 141\n",
      "    Execution count: 141\n",
      "    Successful request count: 141\n",
      "    Avg request latency: 204833 usec (overhead 2 usec + queue 179273 usec + compute input 11 usec + compute infer 25534 usec + compute output 13 usec)\n",
      "\n",
      "Request concurrency: 9\n",
      "  Pass [1] throughput: 39.3333 infer/sec. Avg latency: 230166 usec (std 4387 usec)\n",
      "  Pass [2] throughput: 39 infer/sec. Avg latency: 230771 usec (std 3007 usec)\n",
      "  Pass [3] throughput: 39 infer/sec. Avg latency: 231124 usec (std 3024 usec)\n",
      "  Client: \n",
      "    Request count: 117\n",
      "    Throughput: 39 infer/sec\n",
      "    Avg latency: 231124 usec (standard deviation 3024 usec)\n",
      "    p50 latency: 231094 usec\n",
      "    p90 latency: 235294 usec\n",
      "    p95 latency: 235772 usec\n",
      "    p99 latency: 237769 usec\n",
      "    Avg HTTP time: 230755 usec (send 8 usec + response wait 230746 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 141\n",
      "    Execution count: 141\n",
      "    Successful request count: 141\n",
      "    Avg request latency: 230368 usec (overhead 3 usec + queue 204804 usec + compute input 12 usec + compute infer 25536 usec + compute output 13 usec)\n",
      "\n",
      "Request concurrency: 10\n",
      "  Pass [1] throughput: 39 infer/sec. Avg latency: 255686 usec (std 5665 usec)\n",
      "  Pass [2] throughput: 39 infer/sec. Avg latency: 256604 usec (std 4254 usec)\n",
      "  Pass [3] throughput: 39 infer/sec. Avg latency: 255343 usec (std 2439 usec)\n",
      "  Client: \n",
      "    Request count: 117\n",
      "    Throughput: 39 infer/sec\n",
      "    Avg latency: 255343 usec (standard deviation 2439 usec)\n",
      "    p50 latency: 255137 usec\n",
      "    p90 latency: 258612 usec\n",
      "    p95 latency: 259126 usec\n",
      "    p99 latency: 260394 usec\n",
      "    Avg HTTP time: 255140 usec (send 8 usec + response wait 255130 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 141\n",
      "    Execution count: 141\n",
      "    Successful request count: 141\n",
      "    Avg request latency: 254762 usec (overhead 4 usec + queue 229327 usec + compute input 11 usec + compute infer 25407 usec + compute output 13 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 41 infer/sec, latency 24343 usec\n",
      "Concurrency: 2, throughput: 39 infer/sec, latency 51539 usec\n",
      "Concurrency: 3, throughput: 39.3333 infer/sec, latency 76790 usec\n",
      "Concurrency: 4, throughput: 39 infer/sec, latency 102482 usec\n",
      "Concurrency: 5, throughput: 39 infer/sec, latency 129070 usec\n",
      "Concurrency: 6, throughput: 39 infer/sec, latency 153119 usec\n",
      "Concurrency: 7, throughput: 39 infer/sec, latency 180211 usec\n",
      "Concurrency: 8, throughput: 39 infer/sec, latency 205643 usec\n",
      "Concurrency: 9, throughput: 39 infer/sec, latency 231124 usec\n",
      "Concurrency: 10, throughput: 39 infer/sec, latency 255343 usec\n"
     ]
    }
   ],
   "source": [
    "modelName = \"bertQA-onnx-trt-dynbatch\"\n",
    "maxConcurency= \"10\"\n",
    "batchSize=\"1\"\n",
    "print(\"Running: \"+modelName)\n",
    "!bash ./utilities/run_perf_client_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have observed a fairly dramatic improvement in both latency and throughput. \n",
    "* How big is the impact in comparison to vanilla ONNX configuration or vanilla TorchScript? \n",
    "* What do you think was bottlenecking the multiple instance implementation?\n",
    "\n",
    "Discuss the results with the instructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Congratulations!</h3><br>\n",
    "You've leaned some strategies to improve the GPU utilization and reduce latency using:\n",
    "\n",
    "* Concurrent model execution\n",
    "* Scheduling\n",
    "* Dynamic batching\n",
    "\n",
    "In the next segment of the class we will make a more formal assessment of inference performance across multiple concurrency levels and how to analyze your inference performance in a structured way. Please proceed to the next notebook:<br>\n",
    "[3.0 Server Performance](030_ServerPerformance.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
